{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing an automated digital reader through ML for quality control of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gelquant import gelquant\n",
    "%matplotlib inline\n",
    "\n",
    "#cropped_img =  gelquant.image_cropping(\"Gel7_2019-05-06_Hts103_4_8bit.png\", 70, 200, 1050, 680)\n",
    "#data, bounds = gelquant.lane_parser(cropped_img , 26, 1, 0, 100)\n",
    "\n",
    "# display a sample-image\n",
    "img = Image.open('cleaned_data/Gel2_2019-05-19_Hts105_6_8bit.png')\n",
    "plt.imshow(np.asarray(img))\n",
    "#rotated= img.rotate(-90, resample=Image.BICUBIC, expand=True)\n",
    "#rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up to do a PIL image transformation\n",
    "\n",
    "def find_coeffs(source_coords, target_coords):\n",
    "    matrix = []\n",
    "    for s, t in zip(source_coords, target_coords):\n",
    "        matrix.append([t[0], t[1], 1, 0, 0, 0, -s[0]*t[0], -s[0]*t[1]])\n",
    "        matrix.append([0, 0, 0, t[0], t[1], 1, -s[1]*t[0], -s[1]*t[1]])\n",
    "    A = np.matrix(matrix, dtype=np.float)\n",
    "    B = np.array(source_coords).reshape(8)\n",
    "    res = np.dot(np.linalg.inv(A.T * A) * A.T, B)\n",
    "    return np.array(res).reshape(8)\n",
    "\n",
    "#img = Image.open(sys.argv[1])\n",
    "height, width, channels = np.shape(img)\n",
    "\n",
    "coeffs = find_coeffs(\n",
    "    [(0, 0), (width-30, 0), (width, height-15), (0, height-15)],\n",
    "    [(0, 0), (width, 0), (width, height), (0, height)])\n",
    "\n",
    "img_transform= img.transform((width, height), Image.PERSPECTIVE, coeffs, Image.BICUBIC)\n",
    "plt.imshow(np.asarray(img_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A perspective transformation does help to straighten the reference lane BUT need a non-linear transformation for curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, need to strip out the individual lanes from the 26 lane gel-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [250, 426]\n",
    "number_lanes= 26\n",
    "number_expts= 1\n",
    "\n",
    "bounds = [250, 426]\n",
    "data, bounds = gelquant.lane_parser(img_transform, number_lanes, number_expts, 0, 100)\n",
    "percentages = gelquant.area_integrator(data, bounds, 1, plot_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some Gel-lanes samples  ....\n",
    "\n",
    "plt.plot(data[1])\n",
    "plt.plot(data[5])\n",
    "plt.plot(data[10])\n",
    "plt.plot(data[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design gabor kernels ...\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "\n",
    "# prepare filter bank kernels\n",
    "kernels = []\n",
    "count= 0\n",
    "for sigma in (2, 8):\n",
    "    frequency=0.01\n",
    "    kernel = np.real(gabor_kernel(frequency, theta=0, sigma_x=sigma))\n",
    "    count+=1\n",
    "    plt.plot(kernel)\n",
    "    kernels.append(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feats(data, kernels):\n",
    "    feats = np.zeros((len(kernels),), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(data[1], kernel, mode='wrap')\n",
    "        feats[k, 0] = filtered.mean()\n",
    "        feats[k, 1] = filtered.var()\n",
    "    return feats\n",
    "\n",
    "\n",
    "def match(feats, ref_feats):\n",
    "    min_error = np.inf\n",
    "    min_i = None\n",
    "    for i in range(ref_feats.shape[0]):\n",
    "        error = np.sum((feats - ref_feats[i, :])**2)\n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            min_i = i\n",
    "    return min_i\n",
    "\n",
    "\n",
    "shrink = (slice(0, None, 3), slice(0, None, 3))\n",
    "brick = img_as_float(data[1])[shrink]\n",
    "grass = img_as_float(data[2])[shrink]\n",
    "wall = img_as_float(data[3])[shrink]\n",
    "image_names = ('brick', 'grass', 'wall')\n",
    "images = (brick, grass, wall)\n",
    "\n",
    "# prepare reference features\n",
    "ref_feats = np.zeros((3, len(kernels), 2), dtype=np.double)\n",
    "ref_feats[0, :, :] = compute_feats(brick, kernels)\n",
    "ref_feats[1, :, :] = compute_feats(grass, kernels)\n",
    "ref_feats[2, :, :] = compute_feats(wall, kernels)\n",
    "\n",
    "print('Rotated images matched against references using Gabor filter banks:')\n",
    "\n",
    "print('original: brick, rotated: 30deg, match result: ', end='')\n",
    "feats = compute_feats(ndi.rotate(brick, angle=190, reshape=False), kernels)\n",
    "print(image_names[match(feats, ref_feats)])\n",
    "\n",
    "print('original: brick, rotated: 70deg, match result: ', end='')\n",
    "feats = compute_feats(ndi.rotate(brick, angle=70, reshape=False), kernels)\n",
    "print(image_names[match(feats, ref_feats)])\n",
    "\n",
    "print('original: grass, rotated: 145deg, match result: ', end='')\n",
    "feats = compute_feats(ndi.rotate(grass, angle=145, reshape=False), kernels)\n",
    "print(image_names[match(feats, ref_feats)])\n",
    "\n",
    "\n",
    "def power(image, kernel):\n",
    "    # Normalize images for better comparison.\n",
    "    image = (image - image.mean()) / image.std()\n",
    "    return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +\n",
    "                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)\n",
    "\n",
    "# Plot a selection of the filter bank kernels and their responses.\n",
    "results = []\n",
    "kernel_params = []\n",
    "for theta in (0, 1):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for frequency in (0.1, 0.4):\n",
    "        kernel = gabor_kernel(frequency, theta=theta)\n",
    "        params = 'theta=%d,\\nfrequency=%.2f' % (theta * 180 / np.pi, frequency)\n",
    "        kernel_params.append(params)\n",
    "        # Save kernel and the power image for each image\n",
    "        results.append((kernel, [power(img, kernel) for img in images]))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(5, 6))\n",
    "plt.gray()\n",
    "\n",
    "fig.suptitle('Image responses for Gabor filter kernels', fontsize=12)\n",
    "\n",
    "axes[0][0].axis('off')\n",
    "\n",
    "# Plot original images\n",
    "for label, img, ax in zip(image_names, images, axes[0][1:]):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "for label, (kernel, powers), ax_row in zip(kernel_params, results, axes[1:]):\n",
    "    # Plot Gabor kernel\n",
    "    ax = ax_row[0]\n",
    "    ax.imshow(np.real(kernel), interpolation='nearest')\n",
    "    ax.set_ylabel(label, fontsize=7)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Plot Gabor responses with the contrast normalized for each filter\n",
    "    vmin = np.min(powers)\n",
    "    vmax = np.max(powers)\n",
    "    for patch, ax in zip(powers, ax_row[1:]):\n",
    "        ax.imshow(patch, vmin=vmin, vmax=vmax)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "strt_signl= 250\n",
    "end_signl=  426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gels_df = pd.read_csv('processed_gels.csv', encoding='utf-8')\n",
    "gels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "labels_df = pd.read_excel('cleaned_data/TeachingGels Score Sheet.xlsx', sheetname='Sheet1')\n",
    "labels_df.drop(columns=['Unnamed: 4', 'Score Legend', 'Unnamed: 6'], inplace=True)\n",
    "\n",
    "\n",
    "labels_df['CT_score'].replace('C',0,inplace=True)\n",
    "labels_df['WZ_score'].replace('C',0,inplace=True)\n",
    "labels_df['CT_score'].replace('M',10,inplace=True)\n",
    "labels_df['WZ_score'].replace('M',10,inplace=True)\n",
    "\n",
    "\n",
    "# max of. the 2 columns\n",
    "labels_df['liberal_score'] = labels_df[[\"CT_score\", \"WZ_score\"]].max(axis=1)\n",
    "\n",
    "# min of. the 2 columns\n",
    "labels_df['conservative_score'] = labels_df[[\"CT_score\", \"WZ_score\"]].min(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality= pd.concat([labels_df, gels_df], axis=1)\n",
    "#quality = quality[~quality.liberal_score.str.contains(\"M\")]\n",
    "quality.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINALLY -- ready to build a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "quality.drop(columns=['Gel', 'Lane', 'CT_score', 'WZ_score', 'conservative_score'], axis=1, inplace=True)\n",
    "quality['Target'] = quality['liberal_score']\n",
    "\n",
    "y = quality.Target.values\n",
    "\n",
    "feature_cols = [i for i in list(quality.columns) if i != 'Target']\n",
    "X = quality.ix[:, feature_cols].as_matrix()\n",
    "\n",
    "# Illustrated here for manual splitting of training and testing data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# define method\n",
    "logreg= LogisticRegression(multi_class='multinomial', class_weight='balanced', solver='newton-cg')\n",
    "\n",
    "predicted = cross_val_score(logreg, X_train, y_train, cv=10)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)\n",
    "print(\"Accuracy: %.3f%%\" % (metrics.accuracy_score(y_test, y_pred)*100.0))\n",
    "y_pred = model.predict(X_test)\n",
    "#print(\"F1 Score: \", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "#print(\"Precision Score: \", precision_score(y_test, y_pred, average=\"macro\"))\n",
    "#print(\"Recall Score: \", recall_score(y_test, y_pred, average=\"macro\")) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
    "    #plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "cm_analysis(y_test, y_pred, model.classes_, ymap=None, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "import seaborn as sn      \n",
    "df_cm = pd.DataFrame(cm, index=[\"-2\", \"-1\", \"0\", \"1\", \"2\", \"M\"], columns=[\"-2\", \"-1\", \"0\", \"1\", \"2\", \"M\"])\n",
    "\n",
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets look at average conservative scoring;  evaluate the logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "quality= pd.concat([labels_df, gels_df], axis=1)\n",
    "#quality = quality[~quality.liberal_score.str.contains(\"M\")]\n",
    "\n",
    "quality.drop(columns=['Gel', 'Lane', 'CT_score', 'WZ_score', 'liberal_score'], axis=1, inplace=True)\n",
    "quality['Target'] = quality['conservative_score']\n",
    "\n",
    "y = quality.Target.values\n",
    "\n",
    "\n",
    "feature_cols = [i for i in list(quality.columns) if i != 'Target']\n",
    "X = quality.ix[:, feature_cols].as_matrix()\n",
    "\n",
    "# Illustrated here for manual splitting of training and testing data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# define method\n",
    "logreg= LogisticRegression(class_weight='balanced', solver='newton-cg')\n",
    "\n",
    "predicted = cross_val_score(logreg, X_train, y_train, cv=10)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)\n",
    "print(\"Accuracy: %.3f%%\" % (metrics.accuracy_score(y_test, y_pred)*100.0))\n",
    "y_pred = model.predict(X_test)\n",
    "#print(\"F1 Score: \", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "#print(\"Precision Score: \", precision_score(y_test, y_pred, average=\"macro\"))\n",
    "#print(\"Recall Score: \", recall_score(y_test, y_pred, average=\"macro\")) \n",
    "\n",
    "cm_analysis(y_test, y_pred, model.classes_, ymap=None, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gelquant",
   "language": "python",
   "name": "gelquant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
